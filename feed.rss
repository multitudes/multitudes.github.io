<?xml version="1.0" encoding="UTF-8"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content"><channel><title>Laurent Brusa</title><description>iOS Developer</description><link>https://multitudes.github.io</link><language>en</language><lastBuildDate>Wed, 29 Jan 2020 14:59:29 +0100</lastBuildDate><pubDate>Wed, 29 Jan 2020 14:59:29 +0100</pubDate><ttl>250</ttl><atom:link href="https://multitudes.github.io/feed.rss" rel="self" type="application/rss+xml"/><item><guid isPermaLink="true">https://multitudes.github.io/posts/Delegates-Protocols</guid><title>Delegate Pattern in Swift easy explained</title><description></description><link>https://multitudes.github.io/posts/Delegates-Protocols</link><pubDate>Sat, 25 Jan 2020 09:38:00 +0100</pubDate><content:encoded><![CDATA[<p>Delegation is a very common Design Pattern in iOS. For beginners can be a bit difficult to understand at first. Here is a quick rundown to get it working quickly.</p><p>From the <a href="https://docs.swift.org/swift-book/LanguageGuide/Protocols.html">swift documentation</a>:</p><blockquote><p>Delegation is a design pattern that enables a class or structure to hand off (or delegate) some of its responsibilities to an instance of another type. This design pattern is implemented by defining a protocol that encapsulates the delegated responsibilities, such that a conforming type (known as a delegate) is guaranteed to provide the functionality that has been delegated. Delegation can be used to respond to a particular action or to retrieve data from an external source without needing to know the underlying type of that source.</p></blockquote><p>There is often an analogy with the boss and the intern.</p><p>I made a small example below. Our app has two screens. The first screen changes its colour based on the choice taken by the user on the second screen. How do these two views communicate?</p><p>In this case, we can think of the first screen (the BaseScreen) as our intern waiting to be told what to do. The second screen ( the SelectionScreen) is the boss telling the first screen to change its colour.</p><p>The two ViewColtrollers are as a starting point:</p><h4>The BaseScreen</h4><pre><code><span class="keyword">class</span> BaseScreen: <span class="type">UIViewController</span> {
    <span class="comment">//this button will bring me to the SelectionScreen</span> 
    <span class="keyword">@IBOutlet weak var</span> chooseButton: <span class="type">UIButton</span>!
    
    <span class="keyword">override func</span> viewDidLoad() {
        <span class="keyword">super</span>.<span class="call">viewDidLoad</span>()
    }

    <span class="keyword">@IBAction func</span> chooseButtonTapped(<span class="keyword">_</span> sender: <span class="type">UIButton</span>) {
        <span class="keyword">let</span> selectionVC = storyboard?.<span class="call">instantiateViewController</span>(withIdentifier: <span class="string">"SelectionScreen"</span>) <span class="keyword">as</span>! <span class="type">SelectionScreen</span>
        <span class="call">present</span>(selectionVC, animated: <span class="keyword">true</span>, completion: <span class="keyword">nil</span>)
    }
}
</code></pre><h4>The SelectionScreen</h4><p>This view will show two buttons, pressing one of them will just dismiss the view controller and return to the BaseScreen.</p><pre><code><span class="keyword">class</span> SelectionScreen: <span class="type">UIViewController</span> {

    <span class="keyword">override func</span> viewDidLoad() {
        <span class="keyword">super</span>.<span class="call">viewDidLoad</span>()
    }
    <span class="keyword">@IBAction func</span> redButtonTapped(<span class="keyword">_</span> sender: <span class="type">UIButton</span>) {
        <span class="call">dismiss</span>(animated: <span class="keyword">true</span>, completion: <span class="keyword">nil</span>)
    }

    <span class="keyword">@IBAction func</span> blueButtonTapped(<span class="keyword">_</span> sender: <span class="type">UIButton</span>) {
        <span class="call">dismiss</span>(animated: <span class="keyword">true</span>, completion: <span class="keyword">nil</span>)
    }
}
</code></pre><h4>The steps to implement the delegation pattern are:</h4><ul><li>create a protocol with a function declaration. This function is only declared in the protocol. It will be called in the selection screen and will be implemented in the base screen.</li></ul><pre><code><span class="keyword">protocol</span> ColorChangeDelegate {
    <span class="keyword">func</span> didChooseColor(color: <span class="type">UIColor</span>)
}
</code></pre><ul><li>Our main screen, the base screen, will conform to that delegate protocol and implement the function declared in the protocol.</li></ul><pre><code><span class="keyword">extension</span> <span class="type">BaseScreen</span>: <span class="type">ColorChangeDelegate</span> {
    <span class="keyword">func</span> didChooseColor(color: <span class="type">UIColor</span>) {
        view.<span class="property">backgroundColor</span> = color
    }
}
</code></pre><p>So when the base screen will present the SelectionViewController, it will set itself as its delegate.</p><pre><code><span class="keyword">class</span> BaseScreen: <span class="type">UIViewController</span> {

    <span class="keyword">@IBOutlet weak var</span> chooseButton: <span class="type">UIButton</span>!
    
    <span class="keyword">override func</span> viewDidLoad() {
        <span class="keyword">super</span>.<span class="call">viewDidLoad</span>()
    }

    <span class="keyword">@IBAction func</span> chooseButtonTapped(<span class="keyword">_</span> sender: <span class="type">UIButton</span>) {
        <span class="keyword">let</span> selectionVC = storyboard?.<span class="call">instantiateViewController</span>(withIdentifier: <span class="string">"SelectionScreen"</span>) <span class="keyword">as</span>! <span class="type">SelectionScreen</span>
        <span class="comment">// this is where we assign the base controller as the delegate of the next screen</span> 
        selectionVC.<span class="property">colorDelegate</span> = <span class="keyword">self</span>
        present(selectionVC, animated: <span class="keyword">true</span>, completion: <span class="keyword">nil</span>)
    }
}

</code></pre><ul><li>The selection screen will call the delegate function when the button has been tapped but not implement what will happen in the implementation:</li></ul><pre><code><span class="keyword">class</span> SelectionScreen: <span class="type">UIViewController</span> {
    <span class="comment">// this is the declaration of my delegate. It is unwrapped because it is initialised in the previous screen</span>
    <span class="keyword">var</span> colorDelegate: <span class="type">ColorChangeDelegate</span>!
    
    <span class="keyword">override func</span> viewDidLoad() {
        <span class="keyword">super</span>.<span class="call">viewDidLoad</span>()
    }
    <span class="keyword">@IBAction func</span> redButtonTapped(<span class="keyword">_</span> sender: <span class="type">UIButton</span>) {
    <span class="comment">// if I tap this button I call the method didChooseColor(color:) on my delegate 
    // Guess who is the delegate? Thats right, BaseScreen!</span>
        colorDelegate.<span class="call">didChooseColor</span>(color: .<span class="dotAccess">red</span>)
        <span class="call">dismiss</span>(animated: <span class="keyword">true</span>, completion: <span class="keyword">nil</span>)
    }

    <span class="keyword">@IBAction func</span> blueButtonTapped(<span class="keyword">_</span> sender: <span class="type">UIButton</span>) {
    <span class="comment">// same here as above</span>
        colorDelegate.<span class="call">didChooseColor</span>(color: .<span class="dotAccess">blue</span>)
        <span class="call">dismiss</span>(animated: <span class="keyword">true</span>, completion: <span class="keyword">nil</span>)
    }
}
</code></pre><p>Now tapping on one of the two button will dismiss the SelectionScreen and will change the color on the delegate view which is our BaseScreen.</p><p>This was a very basic explanation. The code is on <a href="https://github.com/multitudes/Delegates-Protocols/tree/master">GitHub</a> I hope this simple example made things a bit clearer about delegation in Swift.</p>]]></content:encoded></item><item><guid isPermaLink="true">https://multitudes.github.io/posts/2019-12-27-What%20I%20learned%20from%20the%20AoC%20for%20Swift%20Playgrounds</guid><title>What I learned from doing the Advent of Code 2019</title><description></description><link>https://multitudes.github.io/posts/2019-12-27-What%20I%20learned%20from%20the%20AoC%20for%20Swift%20Playgrounds</link><pubDate>Fri, 27 Dec 2019 09:38:00 +0100</pubDate><content:encoded><![CDATA[<h1>What I learned from doing the Advent of Code 2019</h1><img src="https://multitudes.github.io/images/aoc/aoc0-sm.png" alt="AoC Image"/><p>At the beginning of the month, I had to study hard for the IHK German Chamber Of Commerce Software developer exams. After passing those I was able to focus on different things again. This is when I found out about the Advent Of Code. To spend more time just writing code and algorithms in the language of my choice has been a great way to relax and have some fun!<br><br>I did many tutorials with Swift before and the experience has always been guided. Of course, it has to be like this, nothing wrong with the tutorials. It is just that I never really did put into practice what I was learning. The advent of code has allowed me to approach and deal with problems. I was almost about to say real-world problems, but really to rescue Santa cannot be considered a real-world problem, can it?</p><p>Let me explain. I love to take a book and practice algorithms. This is a different experience than, say: "I need to calculate this, how can I do it?", and then spending a few hours looking for a solution.</p><p>The <a href="https://adventofcode.com">Advent of code 2019</a> in December 2019 has been a great experience to practice all that I learned with Swift in 2019.</p><p>I enjoyed the challenges and especially the realisations that came with them. The puzzles were not easy for me. I cannot believe 100 people a day could manage to solve those in less than one hour.<br>I loved the Reddit with the discussions and the poems!<br>Not only people get to solve these much quicker than me but they manage to write a poem too. I thought this was wonderful, also I enjoyed to see that people used so many different programming languages to solve the challenges, from Bash, to Go, to Excel, even ARMv7-M Assembly!<br><br>It is incredibly inspiring to see so many skills in actions and look at each other code, even if I do not understand most of it.<br>With time I found two other developers using Swift and the solutions were implemented using packages and templates, allowing the code to be run in Terminal on Mac or Linux. I thought this was interesting because Swift is open source and I still rely too much on the Apple platform.<br><br>This LaurentBrusa article is still in progress and will be for a while, because at the time of writing I did not finish the <a href="https://adventofcode.com">Advent of code 2019</a>. I managed to get to day 15 and then life got in the way.<br><br>I am just not fast enough I guess. In 2020 I will put all my energies to prepare myself for hopefully a new job as a Software developer. Priority are editing my Website, write my CV and apply for jobs. I will put the challenges aside for the moment being!</p><h4>What can go wrong using the Swift Playgrounds on Xcode?</h4><p>My programming language of choice is Swift and I thought the Xcode Playgrounds would be ideal for this. I did not know what would expect me haha! At the end of the article below, I will explain more in detail the pro and contra of using the playgrounds for the challenges.</p><p>I collected all the challenges in one big playground with multiple pages and published it on <a href="https://github.com/multitudes/Advent-of-Code-2019">Github</a> to document my journey.</p><p>Xcode playground can insert markdown and create a nice layout to introduce and describe the challenges and explain the code. They are similar to the Notebooks in Python, and it seemed to be a great feature and a bonus. As seen in the images this is an example of markdown rendering and navigation in the playgrounds. I assigned a shortcut to go from the editing mode to the rendered mode.<br><br>From:</p><img src="https://multitudes.github.io/images/aoc/markdown0.png" alt="AoC Image"/><p>To:</p><img src="https://multitudes.github.io/images/aoc/markdown1.png" alt="AoC Image"/><p>Many WWDC scholarships submission are done with the playgrounds and its use is also actively encouraged by Apple. Swift is a modern language said to be as fast as C and offer many nice features like functional programming. Sounds too good to be true! Let's get started!</p><h4>Day 1</h4><p>This was just to get the party going.<br><br>The leaderboards have been unlocked in 4 minutes 12 seconds, which mean it took so long for the first person to get to the solution.<br>Especially for people starting doing the challenges to learn a new language, the first day is helping to get the system ready, to get comfortable doing all basic operations required by the later challenges like reading and saving files from a disk, display solution, print to screen etc.</p><p>Not difficult at all but it took a long time for me to understand how certain things in Playground works.<br><br>For instance, where do I put the input file? In the Sources or Resources folder? Not a trivial question. The Sources folder is for the source code, but it would work for any other resources too. However, the correct way is to put the input.txt file for the puzzle in Ressources! The Sources folder is for the code. Swift files will not be recognized if put in the wrong folder. This I learned!</p><img src="https://multitudes.github.io/images/aoc/sourceFolder.png" alt="AoC Image"/><h4>Day 2: Meet the IntCode Computer</h4><p>The IntCode Computer is at the heart of many AoC 2019 challenges.<br><br>It starts on day 2:</p><blockquote><p>An Intcode program is a list of integers separated by commas (like 1,0,0,3,99). To run one, start by looking at the first integer (called position 0). Here, you will find an opcode - either 1, 2, or 99. The opcode indicates what to do; for example, 99 means that the program is finished and should immediately halt. Encountering an unknown opcode means something went wrong.For example, suppose you have the following program:1,9,10,3,2,3,11,0,99,30,40,50The first four integers, 1,9,10,3, are at positions 0, 1, 2, and 3. Together, they represent the first opcode (1, addition), the positions of the two inputs (9 and 10), and the position of the output (3). To handle this opcode, you first need to get the values at the input positions: position 9 contains 30, and position 10 contains 40. Add these numbers together to get 70. Then, store this value at the output position; here, the output position (3) is at position 3, so it overwrites itself.</p></blockquote><p>This challenge has been continued in Day 5 where we got more opcodes and in day 7 where we had to use the IntCode Computer to activate out thrusters to reach Santa on time:</p><blockquote><p>The Elves have sent you some Amplifier Controller Software (your puzzle input), a program that should run on your existing Intcode computer. Each amplifier will need to run a copy of the program.</p></blockquote><p>And finally in day 9 came what resulted for many to be a watershed moment in the advent of code:</p><blockquote><p>Your existing Intcode computer is missing one key feature: it needs support for parameters in relative mode.Parameters in mode 2, relative mode, behave very similarly to parameters in position mode: the parameter is interpreted as a position. Like position mode, parameters in relative mode can be read from or written to.The important difference is that relative mode parameters don't count from address 0. Instead, they count from a value called the relative base. The relative base starts at 0.</p></blockquote><p>This challenge somehow came as a shock and became a roadblock for me and many others like is to be seen on the <a href="https://www.reddit.com/r/adventofcode/comments/e85b6d/2019_day_9_solutions/fa9e8av?utm_source=share&utm_medium=web2x">reddit forum</a></p><img src="https://multitudes.github.io/images/aoc/aoc1.png" alt="Reddit Post Image"/><p>The first big problem came on the day 9 part 1 challenge.<br>Many forgot to implement the relative mode on the third parameter, which had been mostly implicitly left out till now.<br>This would give ‘203’ as output. I thought there is an element of genius in how these challenges are made. :)</p><p>I found out and corrected the code. Everything was working correctly now.<br>Perhaps not as elegantly as I wanted but still, this is my first AoC and I am no professional developer after all. Every smaller test file with IntCode was working.. but part two got caught in an infinite loop or so I thought. Nobody else had this problem. I read and reread the challenge description including all possible hints on the AoC website and the Reddit.</p><p>Especially this sentence below had me thinking for a long time.</p><blockquote><p>Part two does have a purpose. I'm not telling exactly what it is yet, but it's an entirely different kind of test than part 1 did. If you got the right answer, your Intcode implementation passed that test. If you got part 1, you should have gotten part 2 for free, but virtual machines are persnickety critters.</p></blockquote><p>and</p><blockquote><p>Once run, it will boost the sensors automatically, but it might take a few seconds to complete the operation on slower hardware.</p></blockquote><p>A few seconds...? I have a mac mini 2018 with an i7 intel 6 cores... It cannot run for 10 minutes?<br><br>I spent hours on this inserting more and more print statements (which ironically would just make my playground slower). I had a bad feeling. Part two would not give me any output and I could not find the bug. I had a break from the IntCode!<br><br>Desperation is a part of doing the challenges. In my case it turned out that the playgrounds as I was using them were too slow!</p><h4>Day 3: Crossed Wires</h4><p>The nice thing about this challenge is the understanding that came with using the right collection type in Swift.<br>For this Sets were the best solution to be quick and efficient.<br>At day 3 I started to refactor code and use a separate swift file for functions and structs in the Sources folder.</p><h4>Day 4: Secure Container</h4><p>Cryptography! Create a sequence of numbers and check how many meet the selected criteria. Good fun, I thought it would be easier then I had an "aha" moment. My solution was wrong. I had to think harder, and this is part of the fun :) I printed the sequences to debug and could see that this slowed down the code quite a bit. I took out the print statements to see if the speed would improve.<br>When writing the this article I actually went back and recorded this gif:</p><img src="https://multitudes.github.io/images/aoc/aocday4-slow.gif" alt="Reddit Post Image"/><p>This is slow! I did not realize at the time how bad this was. I later understood this code would run instantaneously on Swift but not on the playground page... But the worse was yet to come!</p><h4>Day 5 - Sunny with a Chance of Asteroids</h4><p>Again fun times with the IntCode computer!</p><h4>Day 6 - Universal Orbit Map</h4><p>A classic linked lists problem.</p><h4>Day 7 - Amplification Circuit</h4><p>Using the IntCode computer! It gets more and more obvious that who had already at the beginning a modular implementation of the computer will be at advantage!</p><p>I did miss this and did not spend more time at the beginning refactoring this code if I did so the later challenges would have been easier!</p><h4>Day 8: Space Image Format</h4><p>This was neat and not too difficult.</p><h4>Day 9: Sensor Boost</h4><p>Getting dirty!<br>The new feature of the IntCode Computer! &gt; Your existing Intcode computer is missing one key feature: it needs support for parameters in relative mode.</p><p>This will bring some havoc 😄</p><h4>Day 10</h4><p>This challenge incidentally had a great learning effect on me. I had to stop and learn more about angles and radiants including the atan2() function! Great stuff and honestly not trivial, but I am happy that I understood now how to reverse the direction and calculate the angles of my laser beam to destroy those asteroids.</p><h4>Day 11</h4><p>Had me using the IntCode computer to manoeuvre a Hull painting robot. It worked! and that made me happy for the day!</p><h4>Day 12</h4><p>A challenge to determine when 4 Jupyter moons would get into their initial state again. Of course, that had to be done more smartly and I came across a modified version of the greatest common factor algorithm, also called <a href="https://en.wikipedia.org/wiki/Euclidean_algorithm">Euclidean algorithm</a> to calculate the <a href="https://en.wikipedia.org/wiki/Least_common_multiple">Least Common Multiple</a>. This has been a bonus learning experience.</p><blockquote><p>The space near Jupiter is not a very safe place; you need to be careful of a big distracting red spot, extreme radiation, and a whole lot of moons swirling around. You decide to start by tracking the four largest moons: Io, Europa, Ganymede, and Callisto.After a brief scan, you calculate the position of each moon (your puzzle input). You just need to simulate their motion so you can avoid them.Each moon has a 3-dimensional position (x, y, and z) and a 3-dimensional velocity. The position of each moon is given in your scan; the x, y, and z velocity of each moon starts at 0.Simulate the motion of the moons in time steps. Within each time step, first update the velocity of every moon by applying gravity. Then, once all moons' velocities have been updated, update the position of every moon by applying velocity. Time progresses by one step once all of the positions are updated.</p></blockquote><p>However, I had to cross a till now invisible to me and unforeseen barrier with my Xcode Playgrounds. The challenge is about performant code!</p><blockquote><p>Determine the number of steps that must occur before all of the moons' positions and velocities exactly match a previous point in time.For example, the first example above takes 2772 steps before they exactly match a previous point in time; it eventually returns to the initial state.Of course, the universe might last for a very long time before repeating.This set of initial positions takes 4686774924 steps before it repeats a previous state! You might need to find a more efficient way to simulate the universe.</p></blockquote><p>Part 2 of the challenge went into a loop again. All smaller test position has been debugged, the code seemed correct. I spent days on it. Xmas eve was in between so I stopped worrying and had a break. Eventually, after Xmas, I had a go at it again. Again I looked in Reddit for hints. The solution, which again was pure genius and I would probably have missed it, consists of calculating every axe independently, calculate the number of steps for it to go into the original position and velocity, then check the next axe and the next. The result is then the lowest common multiplicator or LCM of the three orbits! I did this but the third orbit would just go on forever. I thought this is wrong. I was growing increasingly frustrated. I thought this is the end of my challenges. I spent hours on it. Until I left it running and made a coffee, then had a phone call. And when I came back the answer was staring at me:</p><blockquote><p>376243355967784</p></blockquote><p>Beautiful but how is that possible? I finally googled specifically for "How slow are playgrounds compared to Swift?" I found this enlightening answer on SO <a href="https://stackoverflow.com/questions/55303853/recursive-algorithm-is-much-slower-in-playgrounds-1-minute-than-xcode-0-1-sec">recursive-algorithm-is-much-slower-in-playgrounds</a> and <a href="https://stackoverflow.com/a/47542545/9497800">swift-playground-execution-speed</a></p><p>Of course, the most performant way is to make all the performance-critical code in a Swift file inside the Sources folder in the playground, but I never imagined this would make such a big difference!</p><p>This below is another example of how the Playgrounds can be unexpectedly quirky!</p><pre><code><span class="keyword">var</span> count = <span class="number">0</span>
<span class="keyword">for</span> i <span class="keyword">in</span> <span class="number">0</span>..&lt;<span class="number">1_000_000_000</span> {
    (count += <span class="number">1</span>) <span class="comment">// try to execute the code with or without the parenthesis! 🤯</span>
    <span class="keyword">if</span> count % <span class="number">100_000</span> == <span class="number">0</span> {
        <span class="comment">// print only every 100_000th loop iteration</span>
        <span class="call">print</span>(count)
    }
}

</code></pre><p>Without the parenthesis: about 10.000 loop iterations per second With parenthesis: about 10.000.000 loop iterations per second !!!</p><h4>Day 13-15</h4><p>After Xmas, I managed to do three more days but the momentum had somewhat gone. I know it is important to get going but I just feel guilty to spend so much time on challenges when I should put my energy into finding a job. This comes with preparing my CV, cleaning up my portfolio and doing a few more iOS project to show. Right now that is all that I can concentrate on. So I stopped on Day 15.</p><h1>Get more Performance!</h1><ul><li>Put all your classes and structs in a swift or in a few .swift files and add them to the sources folder. All the classes and structs need to be marked public or the main code will not see them. This will speed up the performance considerably.</li><li>Use less print statements.</li><li>Use macOs playgrounds (there an option for that in the inspector)</li></ul><h2>The disadvantages to using Xcode Playgrounds for Swift</h2><ul><li>Once you put your classes and structs in sources there is a message that you will get all the time like: "... is inaccessible due to internal protection level". This informs you that you need to make every single property and methods public. That's a lot of extra typing because the structs will need a public initializer. Usually, you would get one for free, well they do need one now.</li><li>Putting the code in the source makes it very slow to debug, the playground needs minutes(!) to understand that you made a change in a Swift file in sources, or that a struct that was in the main file previously, is now in the Sources folder.</li><li>I had to restart Xcode a lot doing the above. Xcode again needs quite a long time to understand that the permission has been changed from internal to public.</li><li>Playgrounds do not have debuggers. An easy option is to use print statements and these make your code slow.</li><li>There is no readLine function in the Playgrounds, so strangely it is not possible to input anything in your running program. All languages since the '70s and before having this possibility obviously, I believe the Playgrounds have always been a way to quickly test some software, so the inputs are hardcoded in the software you write, still, it is puzzling!</li><li>The graphical possibilities using the console are limited. There are other ways to have a graphical interface but this requires some more knowledge of the macOS or iOS UIKit and it is not trivial. For instance, I can draw a maze in the console using the print functions with a carriage return and ASCII characters like "-" and "#" but if I wanted to clear the console? There is no such thing as a clear command and I did look and try! I do miss things which are long been available in Bash and Terminal shells and for a reason! Because they are useful!<ul></ul></li></ul>]]></content:encoded></item><item><guid isPermaLink="true">https://multitudes.github.io/posts/Introducing%20ARKit3</guid><title>Introducing ARKit3</title><description></description><link>https://multitudes.github.io/posts/Introducing%20ARKit3</link><pubDate>Sat, 6 Jul 2019 09:38:00 +0200</pubDate><content:encoded><![CDATA[<p>This is an extract of the Apple Developer Keynote's talk <a href="https://developer.apple.com/videos/play/wwdc2019/604/">Introducing ARKit3</a> for my own enjoyment and learning</p><p align="center">
  <img src="https://multitudes.github.io/images/arkit3/1.png" class="pure-img-responsive" title="GetFollowers"></img>
</p><p>ARKit provides three pillars of functionalities.</p><h3>Tracking</h3><p>It determines where your device is respect to the environment, so that virtual content can be positioned and updated correctly in real time.<br>This creates the illusion that the virtual content is placed in the real world.<br>ARKit provides different tracking technologies such as worldtracking, facetracking and imagetracking.</p><h3>Scene understanding</h3><p>This sits on top of tracking and with it you can identify surfaces, images and 3D Objects in the sceneand attach virtual content right on them.<br>Also learns about the lighting and texture to help make the content more realistic.</p><h3>Rendering</h3><p>Brings the 3D content to life.<br>There are three main renderers: SceneKit, SpriteKit, Metal ...<br><br></p><p align="center">
  <img src="https://multitudes.github.io/images/arkit3/2.png" class="pure-img-responsive" title="GetFollowers"></img>
</p><p>and from this year Reality Kit, designed for AR.<br><br></p><p align="center">
  <img src="https://multitudes.github.io/images/arkit3/3.png" class="pure-img-responsive" title="GetFollowers"></img>
</p><h2>New Features in ARKit3</h2><p>Some of them are Visual Coherence, Positional Tracking, Simultaneous Front and Back Camera, Record and Replay of Sequences, More Robust 3D Object Detection, Multiple-face Tracking, HDR Environment Textures, Faster Reference Image Loading, Motion Capture, Detect up to 100 Images, Face Tracking Enhancements, People Occlusion, Raycasting, Collaborative Session, ML Based Plane Detection, New Plane Classes, RealityKit Integration, AR Coaching UI.</p><h3>People Occlusion</h3><p align="center">
  <img src="https://multitudes.github.io/images/arkit3/4.png" class="pure-img-responsive" title="GetFollowers"></img>
</p><p>(Available on A12 and later)<br><br>Enables virtual content to be rendered behind people and works for multiple people in the scene, for fully and partially visible people and integrates with ARView and ARSCNView.<br><br>To produce a convincing AR experience is important to position the content accurately and also to match the world lighting.<br>When people are in the frame it can quickly break the illusion because when they are in the front, they are expected to cover the model. With people occlusion this problem is solved.<br>Virtual content by default is rendered on top of the camera image. Thanks to machine learning it recognize people placed in the frame and then creates a separate layer including only the pixel representing the people. We call this segmentation. Then we can render that layer on top of everything else. But this would not enough. ARKit uses machine learning to make an additional depth estimation test to understand how far the segmented people are in regard to the camera and to make sure to render only people up front if they are actually closer to the camera. Thanks to the power of the Neural Engine in the A12 chip we are able to do it in every frame in real time.<br><br></p><p align="center">
  <img src="https://multitudes.github.io/images/arkit3/5.png" class="pure-img-responsive" title="GetFollowers"></img>
</p><p>Let's see it in code:<br>We have a new property on ARConfiguration called FrameSemantics.<br><br>This will give you different semantic information of what is in the current frame.</p><pre><code>

<span class="keyword">class</span> ARConfiguration : <span class="type">NSObject</span> {
   <span class="keyword">var</span> frameSemantics: <span class="type">ARConfiguration</span>.<span class="type">FrameSemantics</span> { <span class="keyword">get set</span> }
   <span class="keyword">class func</span> supportsFrameSemantics(<span class="type">ARConfiguration</span>.<span class="type">FrameSemantics</span>) -&gt; <span class="type">Bool</span>
}

</code></pre><p>Specific for People Occlusion there are two methods available:<br>One option is personSegmentation.<br>This is the best when you know people will be always be in the front.</p><pre><code><span class="keyword">let</span> configuration = <span class="type">ARWorldTrackingConfiguration</span>() 
configuration.<span class="property">frameSemantics</span> = .<span class="dotAccess">personSegmentation</span>
session.<span class="call">run</span>(configuration)
</code></pre><p>The other option is person segmentation with depth.<br>This is best if people will be either behind or in the front</p><pre><code><span class="keyword">let</span> configuration = <span class="type">ARWorldTrackingConfiguration</span>() 
configuration.<span class="property">frameSemantics</span> = .<span class="dotAccess">personSegmentationWithDepth</span>
session.<span class="call">run</span>(configuration)
</code></pre><p>For advanced user using Metal you can access the data of the segmentation on every frame like this:</p><pre><code><span class="keyword">open class</span> ARFrame : <span class="type">NSObject</span>, <span class="type">NSCopying</span> {
    <span class="keyword">open var</span> segmentationBuffer: <span class="type">CVPixelBuffer</span>? { <span class="keyword">get</span> } 
    <span class="keyword">open var</span> estimatedDepthData: <span class="type">CVPixelBuffer</span>? { <span class="keyword">get</span> }
}
</code></pre><p>Let's see an example of using the API.<br>In <code>viewDidLoad</code> you create an anchor entity looking for horizontal planes and adding it to a scene.<br>Then you retrieve the <code>url</code> of the model to load and load it using loadModelAsync in asynchronous model.<br>We add the entity as a child of our anchor and also add support for gestures.<br>This will automatically set up a WorldTracking configuration thanks to realityKit.</p><p align="center">
  <img src="https://multitudes.github.io/images/arkit3/6.png" class="pure-img-responsive" title="GetFollowers"></img>
</p><p>I want to implement a toggle that switches occlusion on and off with a simple tap.<br>We need always to check if the device supports FrameSemantic and gracefully handle that case.<br><br></p><p align="center">
  <img src="https://multitudes.github.io/images/arkit3/7.png" class="pure-img-responsive" title="GetFollowers"></img>
</p><p>And add the toggle function<br></p><p align="center">
  <img src="https://multitudes.github.io/images/arkit3/8.png" class="pure-img-responsive" title="GetFollowers"></img>
</p><h2>Motion Capture</h2><p>(Available on A12 and later) Tracks human body in 2D and 3D.<br>You can track the body of a person and skeleton representation which can then be mapped to a virtual character in real time. This is made possible by advanced Machine Learning algorythms</p><h3>2D Body Detection</h3><p>We have a new framesemntics option called .bodyDetection This is supported on the WorldTrackingConfiguration and on the Image and Orientation tracking configuration</p><pre><code><span class="keyword">let</span> configuration = <span class="type">ARWorldTrackingConfiguration</span>() 
configuration.<span class="property">frameSemantics</span> = .<span class="dotAccess">bodyDetection</span>
session.<span class="call">run</span>(configuration)
</code></pre><p>let's have a look at the data you will getting back.</p><p align="center">
  <img src="https://multitudes.github.io/images/arkit3/9.png" class="pure-img-responsive" title="GetFollowers"></img>
</p><p>Every ARFrame delivers an object of type ARBody2D in the dectected body property if a erson was detected. This object contains a 2D Skeleton ARSkeleton2D and it will provide you with all the joints landmarks in normalized image space. They are been returned in a flat hierarchy in an array because this is more efficient for processing, but you also will be getting a skeleton definition and there you have all the information about how to interpret the skeleton data.<br>In particular it contains information about the hierarchy of joints. Like the fact that the hand joint is a child of the elbow joint.</p><h3>3D Motion capture</h3><p align="center">
  <img src="https://multitudes.github.io/images/arkit3/10.png" class="pure-img-responsive" title="GetFollowers"></img>
</p><p>Tracks a human body pose in 3D space and provides a 3D skeleton representation with scale estimation to let you determine the size of the person that is being tracked and it is anchored in world coordinates. We are introducing a new configuration called ARBodyTrackingConfiguration. The frame semantics is turned on by default in that configuration. In addition it trackes device position and orientation and selected worldtrcking features such as plane estimation or image detection. In code:</p><pre><code><span class="keyword">if</span> <span class="type">ARBodyTrackingConfiguration</span>.<span class="property">isSupported</span> {
   <span class="keyword">let</span> configuration = <span class="type">ARBodyTrackingConfiguration</span>()
session.<span class="call">run</span>(configuration) }
</code></pre><p>When ARKit is running and detects a person it will detect a new type of anchor, an ARBodyAnchor. This will provided in the session anchor call back like other anchor types you know. It also has a transform with the position and orientation of the detected person in Worldcoordinates, in addition you get the scale factor and a reference to the 3d skeleton:</p><pre><code><span class="keyword">open class</span> ARBodyAnchor : <span class="type">ARAnchor</span> {
    <span class="keyword">open var</span> transform: <span class="call">simd_float4x4</span> { <span class="keyword">get</span> } 
    <span class="keyword">open var</span> estimatedScaleFactor: <span class="type">Float</span> { <span class="keyword">get</span> } 
    <span class="keyword">open var</span> skeleton: <span class="type">ARSkeleton3D</span> { <span class="keyword">get</span> }
}
</code></pre><p>The yellow joints are the ones which will be delivered to the users with motion capture data. The white ones are leaf joints, additionally available in the skeleton but these are not actively tracked. Labels are available and in the API you can query them by their particular name.</p><p>One particular use case is animate a 3d character</p><h3>Animating 3D Characters</h3><p>You will need a rigged mesh. To do this in code with a realityKit API. First you create an anchor entity of type body and add this anchor to the scene (1) Then load the model (2) and use the asynchronous loading API for that and in the completion handler you will be getting the boidy tracked entity that you just need to add to add as a child to our body anchor. In this way the pose of the keleton will be applied to the model in real time</p><pre><code><span class="comment">// Animating a 3D character with RealityKit

    // Add body anchor (1)</span>
    <span class="keyword">let</span> bodyAnchor = <span class="type">AnchorEntity</span>(.<span class="dotAccess">body</span>)
    arView.<span class="property">scene</span>.<span class="call">addAnchor</span>(bodyAnchor)
    
    <span class="comment">// Load rigged mesh (2)</span>
    <span class="type">Entity</span>.<span class="call">loadBodyTrackedAsync</span>(named: <span class="string">"robot"</span>).<span class="call">sink</span>(receiveValue: { (character) <span class="keyword">in</span>
        
        <span class="comment">// Assign body anchor</span>
        bodyAnchor.<span class="call">addChild</span>(character)
 })

</code></pre><h3>Simultaneous Front and Back Camera</h3><p align="center">
  <img src="https://multitudes.github.io/images/arkit3/11.png" class="pure-img-responsive" title="GetFollowers"></img>
</p><p>Enables World Tracking with face data Enables Face Tracking with device orientation and position Supported on A12 and later</p><p>ARKit lets you do Worldtracking on the back facing camera and face tracking with the true depth system on the front. You can build AR experiences using front and back camera at the same time. And face tracking experiences that make use of the device orientation and position. All this is supported on A12 and later. Example. We run world tracking with plane estimation and also we placed a face mesh on top of the plane and updating it in real time with facial expressions captured through the front camera.</p><p>Lets see the api First we create a world tracking configuration. This will determines which camera stream will be displayed on the screen. This will be the backfacing camera. Now I am turning on the new face tracking enabled property and run the session. This will cause to receive face anchors and I can use any information from that anchor like face mesh and anchor transform etc.<br>Since we are working with world coordinates the user face transform will be placed behind the camera, so in order to visualize the face you will need to transform to a location somewhere in front of the camera.</p><pre><code><span class="comment">// Enable face tracking in world tracking configuration</span>
<span class="keyword">let</span> configuration = <span class="type">ARWorldTrackingConfiguration</span>()
<span class="keyword">if</span> configuration.<span class="property">supportsUserFaceTracking</span> {
}
configuration.<span class="property">userFaceTrackingEnabled</span> = <span class="keyword">true</span>
session.<span class="call">run</span>(configuration)

<span class="comment">// Receive face data</span>
<span class="keyword">func</span> session(<span class="keyword">_</span> session: <span class="type">ARSession</span>, didAdd anchors: [<span class="type">ARAnchor</span>]) {
<span class="keyword">for</span> anchor <span class="keyword">in</span> anchors <span class="keyword">where</span> anchor <span class="keyword">is</span> <span class="type">ARFaceAnchor</span> {
}
...
}
</code></pre><p>Lets see the face tracking configuration. You create the configuration as always and set the worldTrackingEnabled to true. And then you can access in every frame, call back the transform of the current camera position and you can use this for whatever use case you have in mind.</p><pre><code>/ <span class="type">Enable</span> world tracking <span class="keyword">in</span> face tracking configuration
<span class="keyword">let</span> configuration = <span class="type">ARFaceTrackingConfiguration</span>()
<span class="keyword">if</span> configuration.<span class="property">supportsWorldTracking</span> {
}
configuration.<span class="property">worldTrackingEnabled</span> = <span class="keyword">true</span>
session.<span class="call">run</span>(configuration)

<span class="comment">// Access world position and orientation</span>
<span class="keyword">func</span> session(<span class="keyword">_</span> session: <span class="type">ARSession</span>, didUpdate frame: <span class="type">ARFrame</span>) {
<span class="keyword">let</span> transform = frame.<span class="property">camera</span>.<span class="property">transform</span>
...
}
</code></pre><h2>Collaborative Session</h2><p align="center">
  <img src="https://multitudes.github.io/images/arkit3/12.png" class="pure-img-responsive" title="GetFollowers"></img>
</p><p>Before in ARKit two you were able to create multiuser experiences but you had to save the map on one device and send it to another one in order for your user to jump to the same experience. Now with collaborative Session in ARKit 3 you are continuosly sharing the mapping information between multiple devices across the network. This allows to create Ad-hoc multi-user experiences and additionally to share ARAnchors on all devices. All those anchors are identifiable with sessions ID's on all devices At this point all coordinate systems are indipendent from each others even we share the information under the hood.</p><p>In this example two user gather and share feature points in the world space. The two maps merge into each other and will form one map only. Additionally the other user will be shown as ArParticipantAnchor too which will allows you to detect when another user is in your environment. It is not limited to two user but you can have a large amount of users in one session.</p><p>To start in code:</p><pre><code><span class="comment">// Enable a collaborative session with RealityKit
 // Set up networking</span>
    <span class="call">setupMultipeerConnectivity</span>()
 <span class="comment">// Initialize synchronization service</span>
    arView.<span class="property">scene</span>.<span class="property">synchronizationService</span> =
    <span class="keyword">try</span>? <span class="type">MultipeerConnectivityService</span>(session: mcSession)
<span class="comment">// Create configuration and enable the collaboration mode</span>
    <span class="keyword">let</span> configuration = <span class="type">ARWorldTrackingConfiguration</span>()
    configuration.<span class="property">isCollaborationEnabled</span> = <span class="keyword">true</span>
    arView.<span class="property">session</span>.<span class="call">run</span>(configuration)

</code></pre><p>When the collaboration is enabled you will be have a new method on the delegate for you where you will be receiving some data. Upon receiving that data you need to broadcast on the network to other user. Upon reception of the data on all the devices you need to update the AR session so that it knows about the new data.</p><pre><code><span class="comment">// Session callback when some collaboration data is available</span>
<span class="keyword">override func</span> session(<span class="keyword">_</span> session: <span class="type">ARSession</span>, didOutputCollaborationData data:
<span class="type">ARSession</span>.<span class="type">CollaborationData</span>) {
    <span class="comment">// Send collaboration data to other participants</span>
    mcSession.<span class="call">send</span>(data, toPeers: participantIds, with: .<span class="dotAccess">reliable</span>)
}
<span class="comment">// Multipeer Connectivity session delegate callback upon receiving data from peers</span>
<span class="keyword">func</span> session(<span class="keyword">_</span> session: <span class="type">MCSession</span>, didReceive data: <span class="type">Data</span>, fromPeer peerID: <span class="type">MCPeerID</span>) {
    <span class="comment">// Update the session with collaboration data received from another participant</span>
    <span class="keyword">let</span> collaborationData = <span class="type">ARSession</span>.<span class="type">CollaborationData</span>(data)
    session.<span class="call">update</span>(from: collaborationData)
}
</code></pre><h2>AR Coaching UI</h2><p align="center">
  <img src="https://multitudes.github.io/images/arkit3/13.png" class="pure-img-responsive" title="GetFollowers"></img>
</p><p>When you create an AR Experience coaching is really important. You really want to guide your users, whether they are new or returning users. It is not trivial. During the process you need to react to some tracking events.<br>This year we are embedding the guidance in the UIView and we call it AR coaching view.<br>It is a built-in UI overlay you can directly embed in your applications. It guides users to good tracking experience. Provides a consistent design throughout applications. Automaticly activates and deactivates.<br><br>The setup is really simple.<br><br>You need to add it as a child of any UI view, ideally of the ARView.<br>Then connect to the ARSession to the coaching view or connect the session provider outlet to of the coaching view to the session provider itself in case of a story board. Optionally you can specify coaching goals in source code set delegates and disable some functionalities.</p><pre><code>

coachingOverlay.<span class="property">goal</span> = .<span class="dotAccess">horizontalPlane</span>

<span class="comment">// React to activation and deactivation React to relocalization abort request</span>
<span class="keyword">protocol</span> ARCoachingOverlayViewDelegate {
    <span class="keyword">func</span> coachingOverlayViewWillActivate(<span class="type">ARCoachingOverlayView</span>)
    <span class="keyword">func</span> coachingOverlayViewDidDeactivate(<span class="type">ARCoachingOverlayView</span>)
    <span class="keyword">func</span> coachingOverlayViewDidRequestSessionReset(<span class="type">ARCoachingOverlayView</span>)
}
</code></pre><p>This year we have activated and deactivated automatically based on device capabilities Can be explicitly disabled in render options..</p><h2>Face Tracking</h2><p>In ARKit one we enabled face tracking with the ability to track one face. In ARKit 3 we enabled the ability to track three faces concurrently. Also we enabled the ability to track when a face goes out of the screen and returns back giving it the same face ID again. The ID is persistent, but when you start a new session it will be reset.</p><pre><code><span class="keyword">open class</span> ARFaceTrackingConfiguration : <span class="type">ARConfiguration</span> { 
    <span class="keyword">open class var</span> supportedNumberOfTrackedFaces: <span class="type">Int</span> { <span class="keyword">get</span> } 
    <span class="keyword">open var</span> maximumNumberOfTrackedFaces: <span class="type">Int</span>
}
</code></pre><h2>ARPositionalTrackingConfiguration</h2><p>This new tracking configuration is intended for tracking use cases. Maybe you did not need the camera backdrop to be rendered for example.<br>We can achieve a low power consumption with the ability to lower the camera resolution and frame rate.</p><h2>Improvements to the scene understanding</h2><p align="center">
  <img src="https://multitudes.github.io/images/arkit3/14.png" class="pure-img-responsive" title="GetFollowers"></img>
</p><p>Image detection and tracking has been around for some time now. We can now detect up to 100 images at the same time. We also give the ability to detect the size of the printed image for example and adjust the scale accordingly. At runtime we can detect the quality of an image you are passing to ARKit when you wanna create a new reference image. We made improvement to the image detection algorythms with machine learning. In plane estimation with machine learning is more accurate even when feature points are not yet present! Last year we had five different classification, this year we added the ability to detect doors and windows. Plane estimation is really important to place contentsw in the world.</p><pre><code><span class="keyword">class</span> ARPlaneAnchor : <span class="type">ARAnchor</span> {
    <span class="keyword">var</span> classification: <span class="type">ARPlaneAnchor</span>.<span class="type">Classification</span>
}
<span class="keyword">enum</span> ARPlaneAnchor.<span class="type">Classification</span> { 
    <span class="keyword">case</span> wall
    <span class="keyword">case</span> floor 
    <span class="keyword">case</span> ceiling 
    <span class="keyword">case</span> table 
    <span class="keyword">case</span> seat
    <span class="keyword">case</span> door 
    <span class="keyword">case</span> window
}
</code></pre><h2>Raycasting</h2><p>This year with the new raycasting api you can place your content more precisely. It supports every kind of surface alignment not only vertical and horizontal. Also you can track your raycast over time. As you move your device around in real time it can detect more information and place your object on top of the physical surface more accurately as the planes are evolving.</p><p>Start by performing a raycast query. Three parameters. From where to perform the raycast. In the example from the screen center. Then what you want to allow in order to place the content, and the alignment which can be vertical horizontal or any. Then pass the query to the trackedRaycast method which has a call back which allows you to react to the result.. and finally stop it when you are done.</p><pre><code><span class="comment">// Create a raycast query</span>
<span class="keyword">let</span> query = arView.<span class="call">raycastQuery</span>(from: screenCenter,
                                allowing: .<span class="dotAccess">estimatedPlane</span>,
                                alignment: .<span class="dotAccess">any</span>)
<span class="comment">// Perform the raycast</span>
<span class="keyword">let</span> raycast = session.<span class="call">trackedRaycast</span>(query) { results <span class="keyword">in</span>
    <span class="comment">// Refine object position with raycast update</span>
    <span class="keyword">if let</span> result = results.<span class="property">first</span> {
    object.<span class="property">transform</span> = result.<span class="property">transform</span>
    }
}
<span class="comment">// Stop tracking the raycast</span>
 raycast.<span class="call">stop</span>()

</code></pre><h2>Visual Coherence Enhancements</h2><p>Depth of Field effect. The camera on the device always adjust to the environment so the content can now match the depth of field so the object blends perfectly in the environment.<br>Additionally when you move the camera quickly the object get a motion blur. Two new API are HDREnvironmentalTextures and Camera Grain.<br><br>With HDR or high dynamic range you can capture those highlights that make your content more vibrant.<br>Every camera produces some grain and in low light it can be a bit heavier. With this API we can apply those same grain patterns on your virtual contents so it does not stand out.</p><pre><code><span class="keyword">class</span> ARWorldTrackingConfiguration {
    <span class="keyword">var</span> wantsHDREnvironmentTextures: <span class="type">Bool</span> { <span class="keyword">get set</span> }
}
<span class="keyword">class</span> ARFrame {
    <span class="keyword">var</span> cameraGrainIntensity: <span class="type">Float</span> { <span class="keyword">get</span> }
    <span class="keyword">var</span> cameraGrainTexture: <span class="type">MTLTexture</span>? { <span class="keyword">get</span> } }
</code></pre><h2>Record and Replay</h2><p>To develop prototype an AR experience you can record an AR sequence with the reality composer app. You can capture your environment, ARKit will save your sensor data in a movie file container so that you can take it with you and put it in xcode. Now the scheme setting in Xcode has a new seting which allows you to select that file. When that file is selected you can replay that experience. Ideal for prototyping.</p><h2>Sources</h2><p><a href="https://developer.apple.com/videos/play/wwdc2019/604/">WWDC Talk - Introducing ARKit3</a><br><br><a href="https://developer.apple.com/augmented-reality/quick-look/">https://developer.apple.com/augmented-reality/quick-look/</a><br><br><a href="https://developer.apple.com/go/?id=python-usd-library">Download usdz tools</a><br><br>
</p>]]></content:encoded></item><item><guid isPermaLink="true">https://multitudes.github.io/posts/Intro%20to%20ARKit%20with%20a%20simple%20Unity%20tutorial</guid><title>Intro to ARKit in Unity with a simple tutorial</title><description></description><link>https://multitudes.github.io/posts/Intro%20to%20ARKit%20with%20a%20simple%20Unity%20tutorial</link><pubDate>Sat, 20 Apr 2019 09:38:00 +0200</pubDate><content:encoded><![CDATA[<blockquote><p>For the world to be interesting, you have to be manipulating it all the time <br>Brian Eno</br></p></blockquote><h2>Intro to the Apple Augmented Reality framework ARKit</h2><p>ARKit is the Augmented Reality Framework for Apple devices which enables developers to create Augmented Reality Apps for iPhone &amp; iPad. It was introduced along with iOS 11 during WWDC 2017.<br>Anyone using an iOS device that runs on Apple's A9 to A12 Bionic processors (&amp; running iOS 11 or above) can use ARKit apps.<br>The current version is ARKit 2.0 and a new version of ARKit should be out soon, and an update is awaited at the WWDC 2019.</p><h2>How World Tracking Works</h2><p>To create a correspondence between real and virtual spaces, ARKit uses a technique called visual-inertial odometry.<br>This process combines information from the iOS device’s motion sensing hardware with computer vision analysis of the scene visible to the device’s camera.<br>ARKit recognizes notable features in the scene image, tracks differences in the positions of those features across video frames, and compares that information with motion sensing data.<br>The result is a high-precision model of the device’s position and motion. This process can often produce impressive accuracy, leading to realistic AR experiences. However, it relies on details of the device’s physical environment that are not always consistent or are difficult to measure in real time without some degree of error.</p><h2>Create your first ARKit application.</h2><p>In this tutorial, we are going to build a simple AR app in Unity using ARKit 2.0 image tracking feature</p><p>Pre-Requisites are:</p><ul><li>iOS 12 or later on our device</li><li>Xcode 10 or later on our Mac (free download from the App Store)</li><li>The Unity Engine (the free version is fine)</li></ul><p>In this tutorial I will show how to create an simple AR application for your iPhone or iPad in Unity and Xcode. For this you would need a picture both in digital format and printed (to be recognized by your iOS device). And three small mp4 videos in size of max 24Mb each.</p><h4>Install ARKit</h4><p>ARKit needs to be imported into Unity.</p><p>Go to this link:</p><p><a href="https://bitbucket.org/Unity-Technologies/unity-arkit-plugin/downloads/">https://bitbucket.org/Unity-Technologies/unity-arkit-plugin/downloads/</a></p><p>and download the ARKit 2.0 installer.</p><img src="https://multitudes.github.io/images/ARKittutorialscreenshots/13.32.57.png" alt="image"/><p>The image detection feature was introduced already in ARKit 1.5 but did not track the image, it would just trigger the AR experience. Now in ARkit 2.0, you can track the position of image targets as well. If you move the image target any overlay that you have on that image target, whether it's a model or a video, will move along with the image.</p><h4>Open Unity</h4><p>Let's open the unzipped downloaded folder with Unity Hub. I am using <a href="https://unity3d.com/get-unity/download/archive">Unity 2018.3.11f1</a></p><img src="https://multitudes.github.io/images/ARKittutorialscreenshots/13.58.25.png" alt="image"/><p>Click on "Upgrade":</p><img src="https://multitudes.github.io/images/ARKittutorialscreenshots/13.58.41.png" alt="image"/><p>Open the folder UnityARKitPlugin and the Examples folder. In the screenshots, your Unity might look different. Look for the assets folder in your Project Tab.</p><p>Open the folder ARKit1.5 and the UnityARImageAnchor. In this folder double click on the <code>UnityARImageAnchor</code> scene. The Scene will open in the hierarchy panel and screen.</p><p>Open the 'ReferenceImages' folder Drag the image you want to use as an image-trigger in the <code>ReferenceImages</code> folder:</p><img src="https://multitudes.github.io/images/ARKittutorialscreenshots/14.05.40.png" alt="image"/><p>Now go up once in the Folder Hierarchy and click on UnityLogoReferenceImage which is part of the Unity demo. We gonna change the trigger image.</p><img src="https://multitudes.github.io/images/ARKittutorialscreenshots/14.10.02.png" alt="image"/><p>Drag and drop your image from the ReferenceImages folder to the Image Texture field in the Inspector.</p><img src="https://multitudes.github.io/images/ARKittutorialscreenshots/14.10.09.png" alt="image"/><p>In my case, the image is called earth.</p><img src="https://multitudes.github.io/images/ARKittutorialscreenshots/14.12.49.png" alt="image"/><p>Right-click in the hierarchy and select “Create Empty”. You get a Game Object folder. Rename it to “parent”.</p><img src="https://multitudes.github.io/images/ARKittutorialscreenshots/14.14.21.png" alt="image"/><p>Right-click again in the Hierarchy tab and select 3D object and Plane <img src="https://multitudes.github.io/images/ARKittutorialscreenshots/14.16.13.png" alt="image"/></p><p>It looks like this. <img src="https://multitudes.github.io/images/ARKittutorialscreenshots/14.17.01.png" alt="image"/></p><p>Rename Plane to image in the hierarchy</p><p>Reset the position for Plane and parent to 0</p><img src="https://multitudes.github.io/images/ARKittutorialscreenshots/14.17.54.png" alt="image"/><p>Measure the actual real dimension of your image an input it in the scale property Remember they are in meter, so the x will be ex 0.0109 and the z 0.006138</p><img src="https://multitudes.github.io/images/ARKittutorialscreenshots/14.20.32.png" alt="image"/><p>In the ReferenceImages folder right click and create a material for the card and name it image</p><img src="https://multitudes.github.io/images/ARKittutorialscreenshots/14.25.55.png" alt="image"/><p>In the inspector go to legacy shader/ diffuse</p><img src="https://multitudes.github.io/images/ARKittutorialscreenshots/14.28.05.png" alt="image"/><p>Click in the right square texture.. select the image from the popup window</p><img src="https://multitudes.github.io/images/ARKittutorialscreenshots/14.30.49.png" alt="image"/><p>And from the scroll down UI/ default</p><img src="https://multitudes.github.io/images/ARKittutorialscreenshots/14.32.07.png" alt="image"/><p>Now drag the material called image to the image in the hierarchy.</p><img src="https://multitudes.github.io/images/ARKittutorialscreenshots/14.32.51.png" alt="image"/><p>The image selected as image trigger will be visible in the scene on the plane.</p><p>This is how it looks like for me. You can rotate or change things in the inspector if you need...</p><img src="https://multitudes.github.io/images/ARKittutorialscreenshots/14.34.33.png" alt="image"/><p>Drag the image inside the parent</p><img src="https://multitudes.github.io/images/ARKittutorialscreenshots/14.45.06.png" alt="image"/><p>And duplicate this. It is command-D on the Mac to duplicate. They will be one on top of the other so click in the Scene and drag them to where you need them.</p><img src="https://multitudes.github.io/images/ARKittutorialscreenshots/14.47.23.png" alt="image"/><p>It will look like this now</p><img src="https://multitudes.github.io/images/ARKittutorialscreenshots/14.50.14.png" alt="image"/><p>The first will be our trigger image. And then there would be the three videos which will play and move along with the image. Let's attach three videos to these three new planes. Rename them as "1", "2", "3" for instance.</p><img src="https://multitudes.github.io/images/ARKittutorialscreenshots/14.54.08.png" alt="image"/><p>and drag the three videos to the ReferenceImages folder. And then one by one from the folder to the three planes 1,2, and 3 you just created.</p><p>If you click play you should see the three videos playing</p><p>Now drag the parent folder from the hierarchy to the assets and automatically a prefab will be created.</p><p>Delete the parent folder from the hierarchy.</p><p>Select "GenerateImageAnchorCube" in the hierarchy. You need to drag the prefab we just created to the "prefab to generate" field of "GenerateImageAnchorCube" in the inspector view.</p><h4>Build the project in Unity</h4><ul><li>go to file -&gt; build settings,</li><li>You want to edit your bundle id if not already done. click on "Player Settings" in the "Build Settings" window ( bottom left). It will open a new window and the "Bundle Id" will be under "Other Settings"</li></ul><img src="https://multitudes.github.io/images/ARKittutorialscreenshots/211.43.18.png" alt="image"/><ul><li>switch to the iOS platform</li><li>click on add open scenes</li><li>build and save.</li></ul><p>This will create an Xcode build.</p><h4>Open the build in Xcode</h4><p>Open the Unity project build in Xcode and go to images in the assets folder. Go to resources and click on your image. Change the size to cm and the dimension of our physical image. Select your team in General, and make sure that the deployment target is 12.0<br>Choose the device and run and install the app.</p><h3>Sources and links:</h3><p>This tutorial has been inspired by the excellent Video tutorial by <a href="https://youtu.be/POIYPIJtgtM">Parth Anand</a></p><p>About <a href="https://www.theverge.com/tldr/2017/7/26/16035376/arkit-augmented-reality-a-ha-take-on-me-video-trixi-studios">ARKit</a> - The Verge (July 2017)</p><p>Use of ARKit and Procreate App for iPad <a href="https://twitter.com/jaromvogel/status/1125401258494324736">https://twitter.com/jaromvogel</a></p><p>I’ve seen a bunch of ARKit demos that made me think “That’s very cool”. This was the first one that made me think “That’s very useful” <a href="https://twitter.com/madewitharkit">https://twitter.com/madewitharkit</a></p><h4>AR Apps for the iPhone / iPad</h4><p>The App Store offers a curated section to highlight the best AR experiences. Just tap on Apps and scroll down to Top Categories. AR Apps is at the top.</p><p>The <a href="https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=2ahUKEwiF-4O75IviAhUwMewKHRIxAHMQFjAAegQIAhAB&url=https%3A%2F%2Fitunes.apple.com%2Fus%2Fapp%2Fikea-place%2Fid1279244498%3Fmt%3D8&usg=AOvVaw0JuWZkNMjTuCTziRMISM9K">Ikea</a> App has been one of the first demos for ARKit.</p><p>And this is the official apple page for ARKit developers: <a href="https://developer.apple.com/arkit/">https://developer.apple.com/arkit/</a></p><hr>
]]></content:encoded></item><item><guid isPermaLink="true">https://multitudes.github.io/posts/How%20to%20Enable%20Wireless%20Building%20to%20Your%20Phone%20or%20iPad%20in%20Xcode</guid><title>How to Enable Wireless Building in Xcode</title><description></description><link>https://multitudes.github.io/posts/How%20to%20Enable%20Wireless%20Building%20to%20Your%20Phone%20or%20iPad%20in%20Xcode</link><pubDate>Tue, 9 Apr 2019 09:38:00 +0200</pubDate><content:encoded><![CDATA[<p>Many folks dont know or forgot about this interesting feature of Xcode: Wireless Building on device.</p><p>No need for cables, or better said, you will need a cable still just once to set up the wireless syncing.<br>I am using an iPad with a new Mac Mini set up as headless server (without monitor which is being accessed via a MacBook Pro on the same local network).<br>Using Xcode on the Mac Mini is helping me to make use of faster compiling speeds because of his i7 latest generation "Coffee Lake" Intel processor but I would need to connect my device to the Mini which is in another room.<br>Not only that but I want to be able to work on my old Mac Book most of the time with my iPhone and iPad next to me without to be physically attached to the Mini.<br><br>The solution is pretty amazing and I will show you how to do it. It is very easy, and it is quite magical to see your iOS devices suddenly waking up and playing your app.<br>Go in Xcode to the <code>Window</code> menu on the top menu bar. There is a menu item called <code>Devices and Simulators</code>.<br><br><img src="https://multitudes.github.io/images/XcodeWirelessSync/2.png" width="880" title = "Xcode" class = "pure-img-responsive" ><br>

Click on it and you will see the following window opening. There is nothing inside!  

<img src="https://multitudes.github.io/images/XcodeWirelessSync/1.png" width="880" title = "Xcode" class = "pure-img-responsive"><br>

The reason is this is that, for the first sync you will need a cable. So connect the device and try again.  
This time you will see the device showing up!  


<br><img src="https://multitudes.github.io/images/XcodeWirelessSync/3.png" width="880" title = "Xcode test" class = "pure-img-responsive"  ><br>

You need to tick the box `Connect via network` like this:  


<img src="https://multitudes.github.io/images/XcodeWirelessSync/4.png" width="880" title = "Xcode" class = "pure-img-responsive" ><br>

Now you can disconnect the cable and you will be able to build your app on your iOS device without connecting it with a cable! Enjoy.

<hr>
</p>]]></content:encoded></item></channel></rss>