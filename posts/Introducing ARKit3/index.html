<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8" name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="https://unpkg.com/purecss@1.0.1/build/pure-min.css" integrity="sha384-oAOxQR6DkCoMliIh8yFnu25d7Eq/PHS21PClpwjOTeU2jRSq11vu66rf90/cZr47" crossorigin="anonymous"/><link rel="stylesheet" href="https://unpkg.com/purecss@1.0.1/build/grids-responsive-min.css"/><link rel="stylesheet" href="/Pure/styles.css"/><link rel="stylesheet" href="/all.css"/></head><body><div id="layout" class="pure-g"><div><div class="pure-menu pure-menu-horizontal pure-u-1-1 top-header"><a class="pure-menu-heading" href="/">Home</a><ul class="pure-menu-list"><li class="pure-menu-item"><a class="pure-menu-link" href="/about">About</a></li></ul></div></div><div class="sidebar pure-u-1 pure-u-md-1-4"><div class="header"><div id="layout" class="pure-g"><div class="author__avatar"><img src="https://avatars2.githubusercontent.com/u/29355828?s=400&u=8a4658d2d5d4b601e9246f9c77d61cd80b78d848&v=4"/></div><div class="pure-u-md-1-1 pure-u-3-4"><h1 class="brand-title">Laurent Brusa</h1><h3 class="brand-tagline">iOS Developer</h3></div></div><div id="layout" class="pure-g"><div class="pure-u-md-1-1"><a href="https://goo.gl/maps/Ciy1Bxq9EA9JiUgs9"><i class="fas fa-map-marker-alt l-box"></i><a class="social-media" href="https://goo.gl/maps/Ciy1Bxq9EA9JiUgs9">Berlin, Germany</a></a></div><div class="pure-u-md-1-1"><a href="https://github.com/multitudes/portfolio/blob/master/README.md"><i class="fas fa-pepper-hot l-box"></i><a class="social-media" href="https://github.com/multitudes/portfolio/blob/master/README.md">Portfolio</a></a></div><div class="pure-u-md-1-1"><a href="https://www.linkedin.com/in/laurentbrusa"><i class="fab fa-linkedin l-box"></i><a class="social-media" href="https://www.linkedin.com/in/laurentbrusa">LinkedIn</a></a></div><div class="pure-u-md-1-1"><a href="https://github.com/multitudes"><i class="fab fa-github-square l-box"></i><a class="social-media" href="https://github.com/multitudes">GitHub</a></a></div><div class="pure-u-md-1-1"><a href="https://twitter.com/wrmultitudes"><i class="fab fa-twitter-square l-box"></i><a class="social-media" href="https://twitter.com/wrmultitudes">Twitter</a></a></div></div></div></div><div class="content pure-u-1 pure-u-md-3-5 pure-u-xl-6-10"><h2 class="post-title"><a href="/posts/Introducing ARKit3">Introducing ARKit3</a></h2><p class="post-meta">6 July 2019</p><div class="post-tags"><a class="post-category post-category-ios" href="/tags/ios">iOS</a><a class="post-category post-category-developer" href="/tags/developer">developer</a><a class="post-category post-category-augmented reality" href="/tags/augmented-reality">augmented reality</a></div><div class="post-description"><div><p>This is an extract of the Apple Developer Keynote's talk <a href="https://developer.apple.com/videos/play/wwdc2019/604/">Introducing ARKit3</a> for my own enjoyment and learning</p><img src="/images/arkit3/1.png" alt="image"/><p>ARKit provides three pillars of functionalities.</p><h3>Tracking</h3><p>It determines where your device is respect to the environment, so that virtual content can be positioned and updated correctly in real time.<br>This creates the illusion that the virtual content is placed in the real world.<br>ARKit provides different tracking technologies such as worldtracking, facetracking and imagetracking.</p><h3>Scene understanding</h3><p>This sits on top of tracking and with it you can identify surfaces, images and 3D Objects in the sceneand attach virtual content right on them.<br>Also learns about the lighting and texture to help make the content more realistic.</p><h3>Rendering</h3><p>Brings the 3D content to life.<br>There are three main renderers: SceneKit, SpriteKit, Metal ...<br><br><img src="/images/arkit3/2.png" alt="image"/></p><p>and from this year Reality Kit, designed for AR.<br><br><img src="/images/arkit3/3.png" alt="image"/></p><h2>New Features in ARKit3</h2><p>Some of them are Visual Coherence, Positional Tracking, Simultaneous Front and Back Camera, Record and Replay of Sequences, More Robust 3D Object Detection, Multiple-face Tracking, HDR Environment Textures, Faster Reference Image Loading, Motion Capture, Detect up to 100 Images, Face Tracking Enhancements, People Occlusion, Raycasting, Collaborative Session, ML Based Plane Detection, New Plane Classes, RealityKit Integration, AR Coaching UI.</p><h3>People Occlusion</h3><img src="/images/arkit3/4.png" alt="image"/><p>(Available on A12 and later)<br><br>Enables virtual content to be rendered behind people and works for multiple people in the scene, for fully and partially visible people and integrates with ARView and ARSCNView.<br><br>To produce a convincing AR experience is important to position the content accurately and also to match the world lighting.<br>When people are in the frame it can quickly break the illusion because when they are in the front, they are expected to cover the model. With people occlusion this problem is solved.<br>Virtual content by default is rendered on top of the camera image. Thanks to machine learning it recognize people placed in the frame and then creates a separate layer including only the pixel representing the people. We call this segmentation. Then we can render that layer on top of everything else. But this would not enough. ARKit uses machine learning to make an additional depth estimation test to understand how far the segmented people are in regard to the camera and to make sure to render only people up front if they are actually closer to the camera. Thanks to the power of the Neural Engine in the A12 chip we are able to do it in every frame in real time.<br><br><br><img src="/images/arkit3/5.png" alt="image"/></p><p>Let's see it in code:<br>We have a new property on ARConfiguration called FrameSemantics.<br><br>This will give you different semantic information of what is in the current frame.</p><pre><code>

<span class="keyword">class</span> ARConfiguration : <span class="type">NSObject</span> {
   <span class="keyword">var</span> frameSemantics: <span class="type">ARConfiguration</span>.<span class="type">FrameSemantics</span> { <span class="keyword">get set</span> }
   <span class="keyword">class func</span> supportsFrameSemantics(<span class="type">ARConfiguration</span>.<span class="type">FrameSemantics</span>) -&gt; <span class="type">Bool</span>
}

</code></pre><p>Specific for People Occlusion there are two methods available:<br>One option is personSegmentation.<br>This is the best when you know people will be always be in the front.</p><pre><code><span class="keyword">let</span> configuration = <span class="type">ARWorldTrackingConfiguration</span>() 
configuration.<span class="property">frameSemantics</span> = .<span class="dotAccess">personSegmentation</span>
session.<span class="call">run</span>(configuration)
</code></pre><p>The other option is person segmentation with depth.<br>This is best if people will be either behind or in the front</p><pre><code><span class="keyword">let</span> configuration = <span class="type">ARWorldTrackingConfiguration</span>() 
configuration.<span class="property">frameSemantics</span> = .<span class="dotAccess">personSegmentationWithDepth</span>
session.<span class="call">run</span>(configuration)
</code></pre><p>For advanced user using Metal you can access the data of the segmentation on every frame like this:</p><pre><code><span class="keyword">open class</span> ARFrame : <span class="type">NSObject</span>, <span class="type">NSCopying</span> {
    <span class="keyword">open var</span> segmentationBuffer: <span class="type">CVPixelBuffer</span>? { <span class="keyword">get</span> } 
    <span class="keyword">open var</span> estimatedDepthData: <span class="type">CVPixelBuffer</span>? { <span class="keyword">get</span> }
}
</code></pre><p>Let's see an example of using the API.<br>In <code>viewDidLoad</code> you create an anchor entity looking for horizontal planes and adding it to a scene.<br>Then you retrieve the <code>url</code> of the model to load and load it using loadModelAsync in asynchronous model.<br>We add the entity as a child of our anchor and also add support for gestures.<br>This will automatically set up a WorldTracking configuration thanks to realityKit.</p><img src="/images/arkit3/6.png" alt="image"/><p>I want to implement a toggle that switches occlusion on and off with a simple tap.<br>We need always to check if the device supports FrameSemantic and gracefully handle that case.<br><br><img src="/images/arkit3/7.png" alt="image"/></p><p>And add the toggle function<br><img src="/images/arkit3/8.png" alt="image"/></p><h2>Motion Capture</h2><p>(Available on A12 and later) Tracks human body in 2D and 3D.<br>You can track the body of a person and skeleton representation which can then be mapped to a virtual character in real time. This is made possible by advanced Machine Learning algorythms</p><h3>2D Body Detection</h3><p>We have a new framesemntics option called .bodyDetection This is supported on the WorldTrackingConfiguration and on the Image and Orientation tracking configuration</p><pre><code><span class="keyword">let</span> configuration = <span class="type">ARWorldTrackingConfiguration</span>() 
configuration.<span class="property">frameSemantics</span> = .<span class="dotAccess">bodyDetection</span>
session.<span class="call">run</span>(configuration)
</code></pre><p>let's have a look at the data you will getting back.</p><img src="/images/arkit3/9.png" alt="image"/><p>Every ARFrame delivers an object of type ARBody2D in the dectected body property if a erson was detected. This object contains a 2D Skeleton ARSkeleton2D and it will provide you with all the joints landmarks in normalized image space. They are been returned in a flat hierarchy in an array because this is more efficient for processing, but you also will be getting a skeleton definition and there you have all the information about how to interpret the skeleton data.<br>In particular it contains information about the hierarchy of joints. Like the fact that the hand joint is a child of the elbow joint.</p><h3>3D Motion capture</h3><img src="/images/arkit3/10.png" alt="image"/><p>Tracks a human body pose in 3D space and provides a 3D skeleton representation with scale estimation to let you determine the size of the person that is being tracked and it is anchored in world coordinates. We are introducing a new configuration called ARBodyTrackingConfiguration. The frame semantics is turned on by default in that configuration. In addition it trackes device position and orientation and selected worldtrcking features such as plane estimation or image detection. In code:</p><pre><code><span class="keyword">if</span> <span class="type">ARBodyTrackingConfiguration</span>.<span class="property">isSupported</span> {
   <span class="keyword">let</span> configuration = <span class="type">ARBodyTrackingConfiguration</span>()
session.<span class="call">run</span>(configuration) }
</code></pre><p>When ARKit is running and detects a person it will detect a new type of anchor, an ARBodyAnchor. This will provided in the session anchor call back like other anchor types you know. It also has a transform with the position and orientation of the detected person in Worldcoordinates, in addition you get the scale factor and a reference to the 3d skeleton:</p><pre><code><span class="keyword">open class</span> ARBodyAnchor : <span class="type">ARAnchor</span> {
    <span class="keyword">open var</span> transform: <span class="call">simd_float4x4</span> { <span class="keyword">get</span> } 
    <span class="keyword">open var</span> estimatedScaleFactor: <span class="type">Float</span> { <span class="keyword">get</span> } 
    <span class="keyword">open var</span> skeleton: <span class="type">ARSkeleton3D</span> { <span class="keyword">get</span> }
}
</code></pre><p>The yellow joints are the ones which will be delivered to the users with motion capture data. The white ones are leaf joints, additionally available in the skeleton but these are not actively tracked. Labels are available and in the API you can query them by their particular name.</p><p>One particular use case is animate a 3d character</p><h3>Animating 3D Characters</h3><p>You will need a rigged mesh. To do this in code with a realityKit API. First you create an anchor entity of type body and add this anchor to the scene (1) Then load the model (2) and use the asynchronous loading API for that and in the completion handler you will be getting the boidy tracked entity that you just need to add to add as a child to our body anchor. In this way the pose of the keleton will be applied to the model in real time</p><pre><code><span class="comment">// Animating a 3D character with RealityKit

    // Add body anchor (1)</span>
    <span class="keyword">let</span> bodyAnchor = <span class="type">AnchorEntity</span>(.<span class="dotAccess">body</span>)
    arView.<span class="property">scene</span>.<span class="call">addAnchor</span>(bodyAnchor)
    
    <span class="comment">// Load rigged mesh (2)</span>
    <span class="type">Entity</span>.<span class="call">loadBodyTrackedAsync</span>(named: <span class="string">"robot"</span>).<span class="call">sink</span>(receiveValue: { (character) <span class="keyword">in</span>
        
        <span class="comment">// Assign body anchor</span>
        bodyAnchor.<span class="call">addChild</span>(character)
 })

</code></pre><h3>Simultaneous Front and Back Camera</h3><img src="/images/arkit3/11.png" alt="image"/><p>Enables World Tracking with face data Enables Face Tracking with device orientation and position Supported on A12 and later</p><p>ARKit lets you do Worldtracking on the back facing camera and face tracking with the true depth system on the front. You can build AR experiences using front and back camera at the same time. And face tracking experiences that make use of the device orientation and position. All this is supported on A12 and later. Example. We run world tracking with plane estimation and also we placed a face mesh on top of the plane and updating it in real time with facial expressions captured through the front camera.</p><p>Lets see the api First we create a world tracking configuration. This will determines which camera stream will be displayed on the screen. This will be the backfacing camera. Now I am turning on the new face tracking enabled property and run the session. This will cause to receive face anchors and I can use any information from that anchor like face mesh and anchor transform etc.<br>Since we are working with world coordinates the user face transform will be placed behind the camera, so in order to visualize the face you will need to transform to a location somewhere in front of the camera.</p><pre><code><span class="comment">// Enable face tracking in world tracking configuration</span>
<span class="keyword">let</span> configuration = <span class="type">ARWorldTrackingConfiguration</span>()
<span class="keyword">if</span> configuration.<span class="property">supportsUserFaceTracking</span> {
}
configuration.<span class="property">userFaceTrackingEnabled</span> = <span class="keyword">true</span>
session.<span class="call">run</span>(configuration)

<span class="comment">// Receive face data</span>
<span class="keyword">func</span> session(<span class="keyword">_</span> session: <span class="type">ARSession</span>, didAdd anchors: [<span class="type">ARAnchor</span>]) {
<span class="keyword">for</span> anchor <span class="keyword">in</span> anchors <span class="keyword">where</span> anchor <span class="keyword">is</span> <span class="type">ARFaceAnchor</span> {
}
...
}
</code></pre><p>Lets see the face tracking configuration. You create the configuration as always and set the worldTrackingEnabled to true. And then you can access in every frame, call back the transform of the current camera position and you can use this for whatever use case you have in mind.</p><pre><code>/ <span class="type">Enable</span> world tracking <span class="keyword">in</span> face tracking configuration
<span class="keyword">let</span> configuration = <span class="type">ARFaceTrackingConfiguration</span>()
<span class="keyword">if</span> configuration.<span class="property">supportsWorldTracking</span> {
}
configuration.<span class="property">worldTrackingEnabled</span> = <span class="keyword">true</span>
session.<span class="call">run</span>(configuration)

<span class="comment">// Access world position and orientation</span>
<span class="keyword">func</span> session(<span class="keyword">_</span> session: <span class="type">ARSession</span>, didUpdate frame: <span class="type">ARFrame</span>) {
<span class="keyword">let</span> transform = frame.<span class="property">camera</span>.<span class="property">transform</span>
...
}
</code></pre><h2>Collaborative Session</h2><img src="/images/arkit3/12.png" alt="image"/><p>Before in ARKit two you were able to create multiuser experiences but you had to save the map on one device and send it to another one in order for your user to jump to the same experience. Now with collaborative Session in ARKit 3 you are continuosly sharing the mapping information between multiple devices across the network. This allows to create Ad-hoc multi-user experiences and additionally to share ARAnchors on all devices. All those anchors are identifiable with sessions ID's on all devices At this point all coordinate systems are indipendent from each others even we share the information under the hood.</p><p>In this example two user gather and share feature points in the world space. The two maps merge into each other and will form one map only. Additionally the other user will be shown as ArParticipantAnchor too which will allows you to detect when another user is in your environment. It is not limited to two user but you can have a large amount of users in one session.</p><p>To start in code:</p><pre><code><span class="comment">// Enable a collaborative session with RealityKit
 // Set up networking</span>
    <span class="call">setupMultipeerConnectivity</span>()
 <span class="comment">// Initialize synchronization service</span>
    arView.<span class="property">scene</span>.<span class="property">synchronizationService</span> =
    <span class="keyword">try</span>? <span class="type">MultipeerConnectivityService</span>(session: mcSession)
<span class="comment">// Create configuration and enable the collaboration mode</span>
    <span class="keyword">let</span> configuration = <span class="type">ARWorldTrackingConfiguration</span>()
    configuration.<span class="property">isCollaborationEnabled</span> = <span class="keyword">true</span>
    arView.<span class="property">session</span>.<span class="call">run</span>(configuration)

</code></pre><p>When the collaboration is enabled you will be have a new method on the delegate for you where you will be receiving some data. Upon receiving that data you need to broadcast on the network to other user. Upon reception of the data on all the devices you need to update the AR session so that it knows about the new data.</p><pre><code><span class="comment">// Session callback when some collaboration data is available</span>
<span class="keyword">override func</span> session(<span class="keyword">_</span> session: <span class="type">ARSession</span>, didOutputCollaborationData data:
<span class="type">ARSession</span>.<span class="type">CollaborationData</span>) {
    <span class="comment">// Send collaboration data to other participants</span>
    mcSession.<span class="call">send</span>(data, toPeers: participantIds, with: .<span class="dotAccess">reliable</span>)
}
<span class="comment">// Multipeer Connectivity session delegate callback upon receiving data from peers</span>
<span class="keyword">func</span> session(<span class="keyword">_</span> session: <span class="type">MCSession</span>, didReceive data: <span class="type">Data</span>, fromPeer peerID: <span class="type">MCPeerID</span>) {
    <span class="comment">// Update the session with collaboration data received from another participant</span>
    <span class="keyword">let</span> collaborationData = <span class="type">ARSession</span>.<span class="type">CollaborationData</span>(data)
    session.<span class="call">update</span>(from: collaborationData)
}
</code></pre><h2>AR Coaching UI</h2><img src="/images/arkit3/13.png" alt="image"/><p>When you create an AR Experience coaching is really important. You really want to guide your users, whether they are new or returning users. It is not trivial. During the process you need to react to some tracking events.<br>This year we are embedding the guidance in the UIView and we call it AR coaching view.<br>It is a built-in UI overlay you can directly embed in your applications. It guides users to good tracking experience. Provides a consistent design throughout applications. Automaticly activates and deactivates.<br><br>The setup is really simple.<br><br>You need to add it as a child of any UI view, ideally of the ARView.<br>Then connect to the ARSession to the coaching view or connect the session provider outlet to of the coaching view to the session provider itself in case of a story board. Optionally you can specify coaching goals in source code set delegates and disable some functionalities.</p><pre><code>

coachingOverlay.<span class="property">goal</span> = .<span class="dotAccess">horizontalPlane</span>

<span class="comment">// React to activation and deactivation React to relocalization abort request</span>
<span class="keyword">protocol</span> ARCoachingOverlayViewDelegate {
    <span class="keyword">func</span> coachingOverlayViewWillActivate(<span class="type">ARCoachingOverlayView</span>)
    <span class="keyword">func</span> coachingOverlayViewDidDeactivate(<span class="type">ARCoachingOverlayView</span>)
    <span class="keyword">func</span> coachingOverlayViewDidRequestSessionReset(<span class="type">ARCoachingOverlayView</span>)
}
</code></pre><p>This year we have activated and deactivated automatically based on device capabilities Can be explicitly disabled in render options..</p><h2>Face Tracking</h2><p>In ARKit one we enabled face tracking with the ability to track one face. In ARKit 3 we enabled the ability to track three faces concurrently. Also we enabled the ability to track when a face goes out of the screen and returns back giving it the same face ID again. The ID is persistent, but when you start a new session it will be reset.</p><pre><code><span class="keyword">open class</span> ARFaceTrackingConfiguration : <span class="type">ARConfiguration</span> { 
    <span class="keyword">open class var</span> supportedNumberOfTrackedFaces: <span class="type">Int</span> { <span class="keyword">get</span> } 
    <span class="keyword">open var</span> maximumNumberOfTrackedFaces: <span class="type">Int</span>
}
</code></pre><h2>ARPositionalTrackingConfiguration</h2><p>This new tracking configuration is intended for tracking use cases. Maybe you did not need the camera backdrop to be rendered for example.<br>We can achieve a low power consumption with the ability to lower the camera resolution and frame rate.</p><h2>Improvements to the scene understanding</h2><img src="/images/arkit3/14.png" alt="image"/><p>Image detection and tracking has been around for some time now. We can now detect up to 100 images at the same time. We also give the ability to detect the size of the printed image for example and adjust the scale accordingly. At runtime we can detect the quality of an image you are passing to ARKit when you wanna create a new reference image. We made improvement to the image detection algorythms with machine learning. In plane estimation with machine learning is more accurate even when feature points are not yet present! Last year we had five different classification, this year we added the ability to detect doors and windows. Plane estimation is really important to place contentsw in the world.</p><pre><code><span class="keyword">class</span> ARPlaneAnchor : <span class="type">ARAnchor</span> {
    <span class="keyword">var</span> classification: <span class="type">ARPlaneAnchor</span>.<span class="type">Classification</span>
}
<span class="keyword">enum</span> ARPlaneAnchor.<span class="type">Classification</span> { 
    <span class="keyword">case</span> wall
    <span class="keyword">case</span> floor 
    <span class="keyword">case</span> ceiling 
    <span class="keyword">case</span> table 
    <span class="keyword">case</span> seat
    <span class="keyword">case</span> door 
    <span class="keyword">case</span> window
}
</code></pre><h2>Raycasting</h2><p>This year with the new raycasting api you can place your content more precisely. It supports every kind of surface alignment not only vertical and horizontal. Also you can track your raycast over time. As you move your device around in real time it can detect more information and place your object on top of the physical surface more accurately as the planes are evolving.</p><p>Start by performing a raycast query. Three parameters. From where to perform the raycast. In the example from the screen center. Then what you want to allow in order to place the content, and the alignment which can be vertical horizontal or any. Then pass the query to the trackedRaycast method which has a call back which allows you to react to the result.. and finally stop it when you are done.</p><pre><code><span class="comment">// Create a raycast query</span>
<span class="keyword">let</span> query = arView.<span class="call">raycastQuery</span>(from: screenCenter,
                                allowing: .<span class="dotAccess">estimatedPlane</span>,
                                alignment: .<span class="dotAccess">any</span>)
<span class="comment">// Perform the raycast</span>
<span class="keyword">let</span> raycast = session.<span class="call">trackedRaycast</span>(query) { results <span class="keyword">in</span>
    <span class="comment">// Refine object position with raycast update</span>
    <span class="keyword">if let</span> result = results.<span class="property">first</span> {
    object.<span class="property">transform</span> = result.<span class="property">transform</span>
    }
}
<span class="comment">// Stop tracking the raycast</span>
 raycast.<span class="call">stop</span>()

</code></pre><h2>Visual Coherence Enhancements</h2><p>Depth of Field effect. The camera on the device always adjust to the environment so the content can now match the depth of field so the object blends perfectly in the environment.<br>Additionally when you move the camera quickly the object get a motion blur. Two new API are HDREnvironmentalTextures and Camera Grain.<br><br>With HDR or high dynamic range you can capture those highlights that make your content more vibrant.<br>Every camera produces some grain and in low light it can be a bit heavier. With this API we can apply those same grain patterns on your virtual contents so it does not stand out.</p><pre><code><span class="keyword">class</span> ARWorldTrackingConfiguration {
    <span class="keyword">var</span> wantsHDREnvironmentTextures: <span class="type">Bool</span> { <span class="keyword">get set</span> }
}
<span class="keyword">class</span> ARFrame {
    <span class="keyword">var</span> cameraGrainIntensity: <span class="type">Float</span> { <span class="keyword">get</span> }
    <span class="keyword">var</span> cameraGrainTexture: <span class="type">MTLTexture</span>? { <span class="keyword">get</span> } }
</code></pre><h2>Record and Replay</h2><p>To develop prototype an AR experience you can record an AR sequence with the reality composer app. You can capture your environment, ARKit will save your sensor data in a movie file container so that you can take it with you and put it in xcode. Now the scheme setting in Xcode has a new seting which allows you to select that file. When that file is selected you can replay that experience. Ideal for prototyping.</p><h2>Sources</h2><p><a href="https://developer.apple.com/videos/play/wwdc2019/604/">WWDC Talk - Introducing ARKit3</a><br><br><a href="https://developer.apple.com/augmented-reality/quick-look/">https://developer.apple.com/augmented-reality/quick-look/</a><br><br><a href="https://developer.apple.com/go/?id=python-usd-library">Download usdz tools</a><br><br>
</p></div></div></div><div class="footer pure-u-1"><div class="pure-u-1">© 2020 Laurent Brusa</div><div class="pure-u-1">Generated using <a href="https://github.com/johnsundell/publish">Publish</a>. Written in Swift</div><div class="pure-u-1"><a href="/feed.rss">RSS feed</a></div></div></div></body></html>